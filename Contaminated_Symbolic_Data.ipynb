{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding section 1\n",
    "## Let's simulate Newcomb's astronomical observations\n",
    "\n",
    "Let us suppose the \\\"good\\\" observations are normally distributed with mean $ 0 $ and standard deviation $ 10^{-1/2} $, and \\\"bad\\\" observations (from the contaminating distribution) are also normally distributed with mean $ 0 $ but with larger standard deviation $ 5\\cdot10^{-1/2} $.  Let $ \\epsilon=0.05 $, that is, there is a $ 95\\% $ chance we draw a \\\"good\\\" observation and a $ 5\\% $ chance we draw a contaminated observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "epsilon = 0.05 \n",
    "data100 = []\n",
    "#Let's take n=100 samples\n",
    "for j in range(100):\n",
    "    if np.random.binomial(1,epsilon):\n",
    "        data100.append(np.random.normal(0,5*10**(-1/2)))\n",
    "    else:\n",
    "        data100.append(np.random.normal(0,10**(-1/2)))\n",
    "plt.clf()\n",
    "plt.hist(data100,density=True,bins=8)\n",
    "x = np.linspace(min(data100),max(data100),100)\n",
    "plt.plot(x,ss.norm.pdf(x,0,1/np.sqrt(10)),color='green',label='Idealized measurements')\n",
    "\n",
    "print(r'p-value from Wilks-Shapiro test of normality with $n=100$: '+str(ss.shapiro(data100)[1]))\n",
    "#With a sample size of 100 the contaminated data appear almost indistinguishable from truly Gaussian data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm... The data appear consistent with the hypothesis that $P$ is a Gaussian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data500 = []\n",
    "for j in range(500):\n",
    "    if np.random.binomial(1,epsilon):\n",
    "        data500.append(np.random.normal(0,5*10**(-1/2)))\n",
    "    else:\n",
    "        data500.append(np.random.normal(0,10**(-1/2)))\n",
    "plt.clf()\n",
    "plt.hist(data500,density=True,bins=10)\n",
    "x = np.linspace(min(data500),max(data500),100)\n",
    "plt.plot(x,ss.norm.pdf(x,0,1/np.sqrt(10)),color='green',label='Idealized measurements')\n",
    "print(r'p-value from Wilks-Shapiro test of normality with $n=500$: '+str(ss.shapiro(data500)[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a larger sample data can be easily distinguished from Gaussian. How much deviation from the Gaussian assumption is acceptable? What sample size would be required to conclude that $P$ is in fact _not_ Gaussian if only $1\\%$ of observations appear Gaussian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding section 2\n",
    "## Testing the hypothesis that $P$ is independent\n",
    "\n",
    "Given a $2$-dimensional binary source $P$ which has $P((0,0))=P((0,1))=0.1$, $P((1,0))=0.3$, and $P((1,1,))=0.5$, how much data do we need to collect before we can correctly identify that $P$ does not have independent marginals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "P = np.array([[0.1,0.3],[0.1,0.5]])\n",
    "#Let's try a sample of size n=100\n",
    "data100 = np.random.multinomial(100,P.flatten())\n",
    "print(r'p-value for the chi-squared test that $P$ is an independent model when $n=100$: \\\n",
    "'+str(ss.chi2_contingency(data100.reshape((2,2)))[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that the space we are estimating probabilities in only has four outcomes, $100$ samples don't provide compelling enough evidence for us to say that $P$ is not independent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's increase the sample size. Try n=500.\n",
    "data500 = np.random.multinomial(500,P.flatten())\n",
    "ss.chi2_contingency(data500.reshape((2,2)))\n",
    "print(r'p-value for the chi-squared test that $P$ is an independent model when $n=500$: '\\\n",
    "+str(ss.chi2_contingency(data500.reshape((2,2)))[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, this is still not very strong evidence! Let's try collecting an enormous amount of data, $n=1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1000 = np.random.multinomial(1000,P.flatten())\n",
    "print(r'p-value for the chi-squared test that $P$ is an independent model when $n=1000$: '\\\n",
    "+str(ss.chi2_contingency(data1000.reshape((2,2)))[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can say with some confidence that $X$ and $Y$ have some dependence. However, this p-value may not appear that significant after multiple-hypothesis-testing correction (e.g. if we are testing whether the appearance of a purine or pyrimidine in consecutive positions is independent across thousands of transcription factor binding sites)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding section 3\n",
    "## Computing the weight of the independent models in $P$\n",
    "\n",
    "To begin, let's choose a selection of independent models $Q$ so that our set can approximate _any_ independent model. Each independent model can be described by two parameters, $q_x$ and $q_y$ in $[0,1]$, so we decide to place a grid over $q_x$ and $q_y$ and, for each pair of parameters, form the associated independent model (i.e. $Q((0,1))=q_x(1-q_y)$, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's put a 1% grid over the independent distributions on {0,1}^2\n",
    "q_x = np.linspace(0,1,100)\n",
    "q_y = np.linspace(0,1,100)\n",
    "Lambda = 0.0 #This records the largest possible weight for an independent model seen so far\n",
    "Qprime = np.zeros((2,2)) # This records the independent model achieving the largest weight in P\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        Q = np.array([[(1-q_x[i])*(1-q_y[j]),q_x[i]*(1-q_y[j])],[(1-q_x[i])*q_y[j],q_x[i]*q_y[j]]])\n",
    "        #Create the independent source Q with marginals q_x and p_y for each combination of p_x and p_y\n",
    "        if min((P/Q)[np.nonzero(Q)])>Lambda:\n",
    "            Lambda = min((P/Q)[np.nonzero(Q)])\n",
    "            #Compute the latent weight of the single model Q in P, take the largest\n",
    "            Qprime = Q\n",
    "print('Approximate latent weight of independent models in P: '+str(Lambda))\n",
    "print('Approximate largest independent model in P: '+str(Qprime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A surprisingly large amound of data from $P$ can be attributed to the independent model above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding section 4\n",
    "## Examining independence in transcription factor binding sites from JASPAR\n",
    "\n",
    "These data from JASPAR represent 594 ChIP-seq experiments, each for a different transcription factor. Each file has a characteristic $k$ (between $4$ and $26$), the length of the DNA $k$-mer bound by each transcription factor. Then the data from each experiment takes values in $\\{A,C,G,T\\}^k$. Each file has $k$ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binary_alphabet_independent_weight as iw #Contains functions to more explicitly find independent weight\n",
    "import os\n",
    "\n",
    "\n",
    "samples = os.listdir('./TFBSData/')\n",
    "data = []\n",
    "for j in range(len(samples)):\n",
    "    f = open('./TFBSData/'+samples[j],'r')\n",
    "    data.append(f.readlines())\n",
    "    f.close()                      #Reading in k-mers for various transcription factors\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For greater simplicity we don't look at $k$-mers of the $4$ DNA bases, but instead \\\"lump\\\" sources into $k$-mers of purines and pyrimidines (i.e. we look at $k$-dimensional joint sources with a binary alphabet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purPyrBinData = []\n",
    "for j in range(len(data)):\n",
    "    purPyrBinData.append(iw.DNASeq2Phat(data[j])) #For each transcription factor, say data are length d strings\n",
    "                                                    #gives the empirical distribution over {purine,pyrimidine}^d\n",
    "    \n",
    "\n",
    "short_targets = [x for x in purPyrBinData if 4<len(x)<=32] #Look only at 4 and 5-mers (computational efficiency)\n",
    "short_target_indices = [i for i in range(len(purPyrBinData)) if 4<len(purPyrBinData[i])<=32]\n",
    "\n",
    "for i in range(len(short_targets)):\n",
    "    out = iw.numerical_ind_weight(short_targets[i])\n",
    "    print('Independent weight of k-mers bound by '+samples[short_target_indices[i]][:-6]+': '+str(out[0][0]))\n",
    "    s = ''\n",
    "    for j in range(len(out[1])):\n",
    "        s += 'q'+str(j+1)+'='+str(out[1][j])+',  '\n",
    "    s = s[:-3]\n",
    "    print('Optimal Bernoulli parameters for k-mers bound by '+samples[short_target_indices[i]][:-6]+':\\n'+\\\n",
    "         s+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these transcription factors truly appear to bind $k$-mers treating positions as independent in the probability of a purine or pyrimidine in each position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding section 5\n",
    "## Finding the latent weight of the Poisson distributions in $P$\n",
    "\n",
    "This algorithm works by acknowledging that when $\\alpha\\approx0$, $F(\\alpha) = P(0)e^{\\alpha}$. For each other $\\frac{P(i)\\cdot i!}{\\alpha^i\\cdot e^{-\\alpha}}$ (and the \\\"tail\\\" function), we compute the unique $\\alpha$ where it intersects with $P(0)e^\\alpha$. If the function $\\frac{P(i)\\cdot i!}{\\alpha^i\\cdot e^{-\\alpha}}$ intersects with $P(0)e^\\alpha$ at the smallest $\\alpha$ out of all functions, we know that $F(\\alpha)=\\frac{P(i)\\cdot i!}{\\alpha^i\\cdot e^{-\\alpha}}$ in an interval whose left-hand endpoint is that intersection $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import poisson_weight as pois\n",
    "#Bortkiewicz's horse kick data\n",
    "horse_kick_deaths = np.array([144,91,32,11,2]) #Ignore 5+, which have no observations, i.e. L=4\n",
    "mean = (0*144+1*91+2*32+3*11+4*2)/sum(horse_kick_deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a chi-squared goodness-of-fit test to see whether the number of deaths by horse kick appears to have a Poisson distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = sum(horse_kick_deaths)*np.exp(-mean)*np.array([mean**0,mean**1/np.math.factorial(1),mean**2/np.math.factorial(2),\\\n",
    "                                  mean**3/np.math.factorial(3),mean**4/np.math.factorial(4)])\n",
    "print('Maximum likelihood estimate for the Poisson rate of P: '+str(mean))\n",
    "print('p-value of the chi-squared G.O.F. test for Bortkiewicz\\'s data having Poisson distribution: '\\\n",
    "      +str(ss.chisquare(horse_kick_deaths,expected)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this is a classical dataset long-believed to be an example of the Poisson law, it makes sense that it would pass a hypothesis test for Poisson-ness. But what weight do all the Poisson distributions achieve as a component of the source?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = horse_kick_deaths/sum(horse_kick_deaths)\n",
    "print('Weight of Poisson distributions in P: '+str(pois.poisson_weight(P)[0]))\n",
    "print('Rate of the largest Poisson component of P: '+str(pois.poisson_weight(P)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, almost $97\\%$ of samples can be attributed to a Poisson model which should be expected from Bortkiewicz's source. However, this large fraction of samples originate from the Poisson$(0.63)$ distribution, whereas by maximum likelihood we would estimate the source to be a Poisson$(0.7)$ distribution...\n",
    "\n",
    "What if the true source is exactly that of the empirical distribution from Bortkiewicz's data? That is, what if the estimated probabilities are the underlying true source, but we collect a much larger sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_horse_kicks = np.random.multinomial(5000,P) #Resampling the empirical distribution\n",
    "resampled_P = resampled_horse_kicks/sum(resampled_horse_kicks)\n",
    "resampled_mean = np.dot(resampled_horse_kicks,[0,1,2,3,4])/sum(resampled_horse_kicks)\n",
    "resampled_expected = sum(resampled_horse_kicks)*np.exp(-resampled_mean)*np.array([1,resampled_mean,\\\n",
    "                                            resampled_mean**2/2,resampled_mean**3/6,resampled_mean**4/24])\n",
    "print('p-value of the chi-squared G.O.F. test for a resampled version of \\\n",
    "Bortkiewicz\\'s data having Poisson distribution: '+str(ss.chisquare(resampled_horse_kicks,resampled_expected)[1]))\n",
    "\n",
    "print('Poisson weight of the resampled distribution: '+str(pois.poisson_weight(resampled_P)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When the same source is observed through a larger sample our beliefs about its Poisson-ness change dramatically if we use hypothesis tests, but not if we estimate the latent weight of Poisson models!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding section 6\n",
    "## Estimating the exchangeable weight --- how to determine an adequate sample size?\n",
    "\n",
    "Because it is unlikely that a read will contain, say, $20$ CpGs, we focus on \\\"triplets,\\\" sets of $3$ consecutive CpGs whose possible methylation configurations are $(0,0,0),(0,0,1),(0,1,0),(0,1,1),(1,0,0),(1,0,1),(1,1,0),$ and $(1,1,1)$. The configurations of many triplets are fully observed within dozens of reads in a WGBS experiment of typical coverage. \n",
    "\n",
    "In determining what sample size should be used, we want to find a source that is particularly pathological. That is, a source $P$ for which $\\lambda(\\hat P_n)$ has large negative bias and large standard error. Then if a sample size $n$ gives acceptably small bias and standard error in the \\\"worst-case\\\" source, estimates of exchangeable weight should be acceptably accurate for any source.\n",
    "\n",
    "We notice that estimating the exchangeable weight of source which is itself exchangeable tends to give the largest negative bias and standard error. Therefore we place a grid over $\\mathcal Q$, the exchangeable distributions, and test biases and standard errors for various sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(999)\n",
    "\n",
    "def exchangeable_weight(P):#Here, P is a source on {0,1}^3\n",
    "    return P[0]+3*min([P[1],P[2],P[4]])+3*min([P[3],P[5],P[6]])+P[7]\n",
    "\n",
    "E0 = np.array([1,0,0,0,0,0,0,0])\n",
    "E1 = np.array([0,1/3,1/3,0,1/3,0,0,0])\n",
    "E2 = np.array([0,0,0,1/3,0,1/3,1/3,0])\n",
    "E3 = np.array([0,0,0,0,0,0,0,1])\n",
    "\n",
    "import simplex_grid as sg\n",
    "mix_weights = sg.simplex_grid(4,20)/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sources = []\n",
    "for weight in mix_weights:\n",
    "    test_sources.append(weight[0]*E0+weight[1]*E1+weight[2]*E2+weight[3]*E3)\n",
    "    \n",
    "#Is n=30 an adequate sample size?\n",
    "\n",
    "bias = []\n",
    "standard_error = []\n",
    "estimates = []\n",
    "for P in test_sources:\n",
    "    estimate = []\n",
    "    for k in range(100):\n",
    "        Phat = np.random.multinomial(30,P)/30.\n",
    "        estimate.append(exchangeable_weight(Phat))\n",
    "    bias.append(np.mean(estimate)-1)\n",
    "    standard_error.append(np.std(estimate))\n",
    "    estimates.append(estimate)\n",
    "\n",
    "#Let's look at the worst case bias\n",
    "print('Source with worst-case bias: '+str(test_sources[np.argmin(bias)]))\n",
    "plt.hist(estimates[np.argmin(bias)],bins=6,density=True)\n",
    "plt.axvline(1,color='red',linestyle='dashed')\n",
    "plt.xlim(0,1.1)\n",
    "plt.xlabel(r'$\\lambda_{Q}(\\hat P_{30})$')\n",
    "plt.title(r'Worst-case bias estimates, $n=30$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the worst case standard_error\n",
    "print('Source with worst-case standard error: '+str(test_sources[np.argmax(standard_error)]))\n",
    "plt.hist(estimates[np.argmax(standard_error)],density=True,bins=6)\n",
    "plt.axvline(1,color='red',linestyle='dashed')\n",
    "plt.xlim(0,1.1)\n",
    "plt.xlabel(r'$\\lambda_{Q}(\\hat P_{30})$')\n",
    "plt.title(r'Worst-case standard error estimates, $n=30$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly $n=30$ is not enough samples. How about $n=100$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = []\n",
    "standard_error = []\n",
    "estimates = []\n",
    "for P in test_sources:\n",
    "    estimate = []\n",
    "    for k in range(100):\n",
    "        Phat = np.random.multinomial(100,P)/100.\n",
    "        estimate.append(exchangeable_weight(Phat))\n",
    "    bias.append(np.mean(estimate)-1)\n",
    "    standard_error.append(np.std(estimate))\n",
    "    estimates.append(estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Source with worst-case bias: '+str(test_sources[np.argmin(bias)]))\n",
    "plt.hist(estimates[np.argmin(bias)],bins=6,density=True)\n",
    "plt.axvline(1,color='red',linestyle='dashed')\n",
    "plt.xlim(0,1.1)\n",
    "plt.xlabel(r'$\\lambda_{Q}(\\hat P_{100})$')\n",
    "plt.title(r'Worst-case bias estimates, $n=100$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Source with worst-case standard error: '+str(test_sources[np.argmax(standard_error)]))\n",
    "plt.hist(estimates[np.argmax(standard_error)],density=True,bins=6)\n",
    "plt.axvline(1,color='red',linestyle='dashed')\n",
    "plt.xlim(0,1.1)\n",
    "plt.xlabel(r'$\\lambda_{Q}(\\hat P_{100})$')\n",
    "plt.title(r'Worst-case standard error estimates, $n=100$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n=100$ seems like a better sample size but there are still several estimates close to 0.8. We want the estimates to be near $1$ with high probability. How about correcting the bias by bootstrap resampling? Let's try, using the worst-case-bias test source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "P = test_sources[np.argmin(bias)]\n",
    "BS_corrected_estimates = []\n",
    "Lambdas = []\n",
    "BS_lambdas = []\n",
    "for k in range(1000): #Take 1000 samples of size n=100 from the pathological source P\n",
    "    Phat = np.random.multinomial(100,P)/100.\n",
    "    Lambda = exchangeable_weight(Phat) #Compute the exchangeable weight from the empirical measure\n",
    "    Lambdas.append(Lambda)\n",
    "    BS_estimates = []\n",
    "    for j in range(1000):\n",
    "        Phat_star = np.random.multinomial(100,Phat)/100. #Resample the empirical measure, taking 100 observations\n",
    "        BS_estimates.append(exchangeable_weight(Phat_star)) #Compute the exchangeable weight of the \n",
    "                                                            #empirical measure associated with this resample\n",
    "    BS_lambdas.append(BS_estimates)\n",
    "\n",
    "corrected_lambdas = []\n",
    "for j in range(100):\n",
    "    corrected_lambdas.append(2*Lambdas[j]-np.mean(BS_lambdas[j]))\n",
    "plt.hist(Lambdas,density=True,bins=6,alpha=0.5,color='red',label=r'Uncorrected $\\hat\\lambda_{100}$')\n",
    "plt.hist(corrected_lambdas,density=True,bins=6,alpha=0.5,color='green',label=r'BS-corrected $\\hat\\lambda_{100}$')\n",
    "plt.legend(loc='upper left')\n",
    "plt.axvline(1,color='red',linestyle='dashed')\n",
    "plt.xlim(0,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well can we estimate the exchangeable weight on a more typical (less pathological source)? Let's pick up a source uniformly at random from the set of distributions on a triplet's methylation configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.random.dirichlet([1]*8)#Choosing a source uniformly at random from P({0,1}^3)\n",
    "Lambda = exchangeable_weight(P)\n",
    "Lambda_hats = []\n",
    "for j in range(1000):\n",
    "    Phat = np.random.multinomial(100,P)/100.\n",
    "    Lambda_hat = exchangeable_weight(Phat)\n",
    "    resampled_lambdas = []\n",
    "    for j in range(1000):\n",
    "        Phat_star = np.random.multinomial(100,Phat)/100.\n",
    "        Lambda_star = exchangeable_weight(Phat_star)\n",
    "        resampled_lambdas.append(Lambda_star)\n",
    "    Lambda_hats.append(2*Lambda_hat-np.mean(resampled_lambdas))\n",
    "plt.xlim(0,1)\n",
    "plt.hist(Lambda_hats,density=True,bins=20)\n",
    "plt.axvline(Lambda,color='red',linestyle='dashed')\n",
    "plt.xlabel(r'$\\lambda_{Q}(\\hat P_100)$, corrected by bootstrap bias')\n",
    "plt.title('Estimating the exchangeable weight of a random source')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like $n=100$ may not be such a bad sample size for estimating the exchangeable weight of most sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
